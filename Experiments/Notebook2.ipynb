{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 2: Dimensionality Reduction using Gaussian Processes\n",
    "\n",
    "The aim of this lab session is to learn about GPLVM using GPy. The online documentation of GPy is available [here](http://gpy.readthedocs.org/en/latest/). We will focus on three aspects of GPs: \n",
    "* Prinicipal Component Analysis\n",
    "* Gaussian Process Latent Variable Model\n",
    "* Modeling Covariance Functions for GPLVM\n",
    "* Bayesian Gaussian Process Latent Variable Models\n",
    "* Manifold Relevance Determination\n",
    "\n",
    "Source: [Gaussian Process Summer School 2015](http://gpss.cc/gpss15/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import python modules\n",
    "import GPy\n",
    "import string\n",
    "import numpy as np\n",
    "import cPickle as pickle\n",
    "from GPy.util import pca\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# call matplotlib with the inline command to make plots appear within the browser\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code is utility functions for plotting and to prepare the bigger models for later usage. If you are interested, you can have a look, but this is not essential."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "colors = [\"#3FCC94\", \"#DD4F23\", \"#C6D63B\", \"#D44271\", \n",
    "          \"#E4A42C\", \"#4F9139\", \"#6DDA4C\", \"#85831F\", \n",
    "          \"#B36A29\", \"#CF4E4A\"]\n",
    "def plot_model(X, which_dims, labels):\n",
    "    fig = plt.figure()\n",
    "    ax = fig.add_subplot(111)\n",
    "    \n",
    "    X = X[:,which_dims]\n",
    "    \n",
    "    ulabs = []\n",
    "    for lab in labels:\n",
    "        if not lab in ulabs:\n",
    "            ulabs.append(lab)\n",
    "            pass\n",
    "        pass\n",
    "    for i, lab in enumerate(ulabs):\n",
    "        ax.scatter(*X[labels==lab].T,marker='o',color=colors[i],label=lab)\n",
    "        pass\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Download Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this lab, we use a dataset `digits.npy` containing all handwritten digits from $0 \\cdots 9$ handwritten, provided by deCampos et al. [2009]. We will only use some of the digits for the demonstrations in this lab class, but you can edit the code below to select different subsets of the digit data as you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# choose subset of digits to work on\n",
    "which = [0,1,2,6,7,9] \n",
    "data = np.load('digits.npy')\n",
    "data = data[which,:,:,:]\n",
    "num_classes, num_samples, height, width = data.shape\n",
    "\n",
    "# get the digits data and corresponding labels\n",
    "Y = data.reshape((data.shape[0]*data.shape[1],data.shape[2]*data.shape[3]))\n",
    "lbls = np.array([[l]*num_samples for l in which]).reshape(Y.shape[0], 1)\n",
    "labels = np.array([[str(l)]*num_samples for l in which])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can try to plot some of the digits using `plt.matshow` (the digit images have size `16x16`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(Y[0,:].reshape((height,width)), cmap=plt.cm.gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Principal Component Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Principal component analysis (PCA) finds a rotation of the observed outputs, such that the rotated principal component (PC) space maximizes the variance of the data observed, sorted from most to least important (most to least variable in the corresponding PC)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# it is important to normalize the data\n",
    "Yn = Y-Y.mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now letâ€™s run PCA on the reshaped dataset $\\mathbf{Y}$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "p = pca.PCA(Yn) # create PCA class with digits dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting plota will show the lower dimensional representation of the digits in 2 dimensions,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot first 20 eigenvalue fractions\n",
    "p.plot_fracs(20) \n",
    "\n",
    "# plot the latent space by PCA\n",
    "p.plot_2d(Y,labels=labels.flatten(), colors=colors)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Gaussian Process Latent Variable Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian Process Latent Variable Model (GP-LVM) embeds PCA into Gaussian process framework. In the Gaussian Process, any covariance function can be used. One example is the automatic relevance determination (ARD) linear kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set latent dimensions to use\n",
    "input_dim = 4 \n",
    "\n",
    "# define covariance function to be used\n",
    "kernel = GPy.kern.Linear(input_dim, ARD=True)\n",
    "\n",
    "# setup the GPLVM model\n",
    "m = GPy.models.GPLVM(Yn, input_dim=input_dim, kernel=kernel)\n",
    "\n",
    "# optimize for 1000 iterations\n",
    "m.optimize(messages=1, max_iters=1000) \n",
    "\n",
    "# visualize the ARD kernel parameters\n",
    "m.kern.plot_ARD()\n",
    "\n",
    "# visualize the two most significant dimensions\n",
    "plot_model(m.X, m.linear.variances.argsort()[-2:], labels.flatten())\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the linear covariance, these latent points can be optimized with an eigenvalue problem, but generally, for non-linear covariance functions, we have to use gradient based optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) How do your linear solutions differ between PCA and GPLVM with a linear kernel? Look at the plots and also try and consider how the linear ARD parameters compare to the eigenvalues of the principal components."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) The next step is to use a non-linear mapping between inputs $\\mathbf{X}$ and ouputs $\\mathbf{Y}$ by selecting the RBF (`GPy.kern.rbf`) covariance function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set latent dimensions to use\n",
    "input_dim = 4 \n",
    "\n",
    "# define covariance function to be used\n",
    "kernel = GPy.kern.RBF(input_dim, ARD=True)\n",
    "\n",
    "# setup the GPLVM model\n",
    "m = GPy.models.GPLVM(Yn, input_dim=input_dim, kernel=kernel)\n",
    "\n",
    "# optimize for 1000 iterations\n",
    "m.optimize(messages=1, max_iters=1000) \n",
    "\n",
    "# visualize the ARD kernel parameters\n",
    "m.kern.plot_ARD()\n",
    "\n",
    "# visualize the two most significant dimensions\n",
    "plot_model(m.X, m.rbf.lengthscale.argsort()[:2], labels.flatten())\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) How does the nonlinear model differe from the linear model? Are there digits that the GPLVM with an exponentiated quadratic covariance can separate, which PCA is not able to?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Try modifying the covariance function and running the model again. For example you could try a combination of the linear and exponentiated quadratic covariance function or the Matern 5/2. If you run into stability problems try initializing the covariance function parameters differently."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set latent dimensions to use\n",
    "input_dim = 4 \n",
    "\n",
    "# define covariance function to be used\n",
    "kernel = GPy.kern.Matern52(input_dim, ARD=True)+GPy.kern.Linear(input_dim, ARD=True)+GPy.kern.RBF(input_dim, ARD=True)\n",
    "\n",
    "# setup the GPLVM model\n",
    "m = GPy.models.GPLVM(Yn, input_dim=input_dim, kernel=kernel)\n",
    "\n",
    "# optimize for 1000 iterations\n",
    "m.optimize(messages=1, max_iters=1000) \n",
    "\n",
    "# visualize the ARD kernel parameters\n",
    "m.kern.plot_ARD()\n",
    "\n",
    "# visualize the two most significant dimensions\n",
    "plot_model(m.X, m.sum.Mat52.lengthscale.argsort()[:2], labels.flatten())\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 Bayesian Gaussian Process Latent Variable Model\n",
    "\n",
    "In GPLVM we use a point estimate of the latent space $\\mathbf{X}$. In the Bayesian GPLVM we approximate the true distribution $p(\\mathbf{X}|\\mathbf{Y})$ by a variational approximation $q(\\mathbf{X})$ and integrate $\\mathbf{X}$ out. \n",
    "\n",
    "Handling the uncertainty in a principled way allows the model to make an assessment of whether a particular latent dimension is required, or the variation is better explained by noise. This allows the algorithm to switch off latent dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set number of latent dimensions to use\n",
    "input_dim = 5 \n",
    "\n",
    "# define ARD kernel\n",
    "kern = GPy.kern.RBF(input_dim,ARD=True) \n",
    "\n",
    "# initialize model\n",
    "m = GPy.models.BayesianGPLVM(Yn, input_dim=input_dim, kernel=kern, num_inducing=25)\n",
    "\n",
    "# initialize noise as 1% of variance in data\n",
    "m.likelihood.variance = m.Y.var()/100.\n",
    "\n",
    "# optimize the model\n",
    "m.optimize(messages=1)\n",
    "\n",
    "# visualize the ARD kernel parameters\n",
    "m.kern.plot_ARD()\n",
    "\n",
    "# visualize the two most significant dimensions\n",
    "plot_model(m.X.mean, m.rbf.lengthscale.argsort()[:2], labels.flatten())\n",
    "plt.legend()\n",
    "\n",
    "# save the model\n",
    "m.pickle('digits.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) How does the Bayesian GP-LVM compare with the standard model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manifold Relevance Determination\n",
    "\n",
    "In Manifold Relevance Determination we try to find one latent space, common for $K$ observed output sets (modalities) $\\{\\mathbf{Y}_{k}\\}_{k=1}^{K}$. Each modality is associated with a separate set of ARD parameters so that it switches off different parts of the whole latent space and, therefore, $\\mathbf{X}$ is softly segmented into parts that are private to some, or shared for all modalities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# run mrd example model\n",
    "m = GPy.examples.dimensionality_reduction.mrd_simulation(optimize = False, plot=False)\n",
    "\n",
    "# optimize the model\n",
    "m.optimize(messages = True, max_iters=3e3, optimizer = 'bfgs')\n",
    "\n",
    "# plot the latent dimensions\n",
    "_ = m.X.plot()\n",
    "\n",
    "# plot the scales\n",
    "m.plot_scales()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Which signal is shared across the three datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Which are private?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Are there signals shared only between two of the three datasets?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "C. M. Bishop. Pattern recognition and machine learning, volume 1. springer New York, 2006.\n",
    "\n",
    "T. de Campos, B. R. Babu, and M. Varma. Character recognition in natural images. VISAPP 2009.\n",
    "\n",
    "N. D. Lawrence. Probabilistic non-linear principal component analysis with Gaussian process latent variable models. In Journal of Machine Learning Research 6, pp 1783--1816, 2005\n",
    "\n",
    "M. K. Titsias, and N. D. Lawrence. Bayesian Gaussian Process Latent Variable Model. AISTATS. Vol. 9. 2010.\n",
    "\n",
    "A. Damianou, M. K. Titsias, and N. D. Lawrence. Manifold relevance determination. arXiv preprint arXiv:1206.4610 (2012)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {
    "065d8b42f2154344a6987f203af08cb1": {
     "views": []
    },
    "087a109b09184af9bae042f7501e49bf": {
     "views": []
    },
    "0af620152cae45e1842cea70ff9d70a4": {
     "views": []
    },
    "0b590294154e464d8e49538b22145293": {
     "views": []
    },
    "0f111d760d4343d4847da8bc98aacac5": {
     "views": []
    },
    "0f22482e087a48bd9ff41d61f48a673b": {
     "views": []
    },
    "11a3ac57906a41bdaf0a929452360ad2": {
     "views": []
    },
    "186fe4e253de44acb4c2a5d8ed025b75": {
     "views": []
    },
    "1c88613cb9e34ba48c444a3c6ffa0e29": {
     "views": []
    },
    "1cb5c4a31b5a4a328a175310736ef6ba": {
     "views": []
    },
    "1e62a2479c5549cfbfd22b2da827e3ca": {
     "views": []
    },
    "20ddc914eb6b4db78d4d68e6f111503d": {
     "views": []
    },
    "26ea9fc96e3f4c1cbd8df70c26615f36": {
     "views": []
    },
    "28de368772a5407ab8a539f87c4bc496": {
     "views": []
    },
    "2a140501fe684d91a98d6215e333640c": {
     "views": []
    },
    "2b9dd310e7c1405fad00b32142cd2ebe": {
     "views": []
    },
    "2c5c5014fe3e406eb45481f434ede3cd": {
     "views": []
    },
    "32c603d6274743a29592e5048894eb4f": {
     "views": []
    },
    "3b1f59e1494b44549442717bfb360c48": {
     "views": []
    },
    "3be9e05ffbd142349a375f185d5d039d": {
     "views": []
    },
    "3c42ccb4f85943ad898f8b6693c64a72": {
     "views": []
    },
    "3d3476f1123c496aa86af3abf23a747b": {
     "views": []
    },
    "4f16ac4e69cd4b678861c367c8be849a": {
     "views": []
    },
    "52857c934f44492cbf82a099f3b065f3": {
     "views": []
    },
    "55c27beb063f4e728e16f879d7512d3d": {
     "views": []
    },
    "58e36d3dd8d04a969b2551743eb4f7dd": {
     "views": []
    },
    "59e070143e2947618e653a25e31adf32": {
     "views": []
    },
    "5bff75028157433397ac40953532b31c": {
     "views": []
    },
    "62a9021599b44c56b852439a40c8f33d": {
     "views": []
    },
    "6550c8c72111465d81ad25ed5dc5f6ae": {
     "views": []
    },
    "65a7d6d093b747bf969abfaf1e18c14f": {
     "views": []
    },
    "691b49371a0d438fa9c8649d111f71fa": {
     "views": []
    },
    "6a9e0c42cc454de899f7bcbc4e89c4e2": {
     "views": []
    },
    "70df15c3d56d48feaec0537988163bc2": {
     "views": []
    },
    "74942d8d271e4516b9fdad5b34ca1355": {
     "views": []
    },
    "7610550b0300458aa2d8813a3f618b70": {
     "views": []
    },
    "797e88dd21bc4e019b66c044e8271d40": {
     "views": []
    },
    "84b56d71e6fb489ca85e0a71eedae212": {
     "views": []
    },
    "865734fecc534364ba47f9eacc61e983": {
     "views": []
    },
    "8f23e99446ef42afbaa334c592a86de6": {
     "views": []
    },
    "9323d2d11a7645dbb2d1143a51b4afe8": {
     "views": []
    },
    "9400c9b7ca1947eeac6a1eec8331cf5e": {
     "views": []
    },
    "94beac5ae6ad4d33a7979bd4a026d287": {
     "views": []
    },
    "9d1f1c259b2a421c9eee2a43d2fd8ca5": {
     "views": []
    },
    "9f3f650f730149e4a67340b09e1781b7": {
     "views": []
    },
    "a1c214776a24426ca9e5e32f536f7773": {
     "views": []
    },
    "a7326f09f72b405990cea96ae8efcc5a": {
     "views": []
    },
    "acc5b88e01144315a47ee2e63872e013": {
     "views": []
    },
    "ad4353ee8cc442cbac20be35f635e29f": {
     "views": []
    },
    "af20499fe86344b5873e8994513c58f4": {
     "views": []
    },
    "b968455a83c74fad95b54f8a38ea0e70": {
     "views": []
    },
    "bb6cedb613174c949707710cb6661721": {
     "views": []
    },
    "be8b2bd8823945eda6f748ac6a139578": {
     "views": []
    },
    "bf2bfef2684f4e919b27219dbf26872f": {
     "views": []
    },
    "c4ba62e10c4044bebaa09c292424bec3": {
     "views": []
    },
    "c4d69f9fe32c4cfe907acd1c33f9ecf2": {
     "views": []
    },
    "c5dc4b3f202f402facdd2c085995f03f": {
     "views": []
    },
    "c630916563944a2695a9bc7a4dcf1c68": {
     "views": []
    },
    "c6d6e5898c854260afe7824835804101": {
     "views": []
    },
    "cb14648ce6c949a2b0fd47cdffd169f7": {
     "views": []
    },
    "cba658d0f8af46fc91a81ebf5624fb79": {
     "views": []
    },
    "cc492617987241b2a4dfc7af57183e42": {
     "views": []
    },
    "cc777dd2ade840beaf7c3e974995bb62": {
     "views": []
    },
    "cea8d65924c4429fbd5a342b023f9747": {
     "views": []
    },
    "cfecabc376ec4d88babfc644581e8e42": {
     "views": []
    },
    "d148de2e6af74fbeb450395b24291955": {
     "views": []
    },
    "e0e2a74dbaa449bdb9d5125536e4dd6a": {
     "views": []
    },
    "e26f9cf30f9341768544a2c37184ff6d": {
     "views": []
    },
    "f2d2efec65814820a8682542d6fcde4d": {
     "views": []
    },
    "f571df479b3549388f23efa4d0224a7c": {
     "views": []
    },
    "ff569323f3394651a15226cf611c615b": {
     "views": []
    },
    "ffeb87cf9adf478abeeb5a856ac77655": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
