{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab session 1: Gaussian Process models with GPy\n",
    "\n",
    "Source: [Gaussian Process Summer School 2015](http://gpss.cc/gpss15/)\n",
    "\n",
    "The aim of this lab session is to get us started with GPy Library. The current draft of the online documentation of GPy is available from [this page](http://gpy.readthedocs.org/en/latest/). We will focus on three aspects of GPs: the kernel, the random sample paths and the GP regression model.\n",
    "\n",
    "Requirements:\n",
    "* [GPy](https://github.com/SheffieldML/GPy): Installation instructions available on the homepage.\n",
    "* [Scipy Stack](https://www.scipy.org/index.html): This includes numpy, matplotlib and Ipython. Installation can be done using `pip`:\n",
    "```\n",
    "(sudo) pip install numpy --upgrade\n",
    "(sudo) pip install jupyter --upgrade\n",
    "(sudo) pip install matplotlib --upgrade\n",
    "(sudo) pip install ipython[all] --upgrade\n",
    "```\n",
    "The `sudo` is optional if you want to have installation in the root folder when working in the Linux operating system. It should not be used for Anaconda or when working in the Windows operating system.\n",
    "\n",
    "* [Anaconda](https://www.continuum.io/downloads): Necessary for Windows, optional for Linux."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Windows Operating System:\n",
    "* Install Anaconda by downloading from this [link](https://www.continuum.io/downloads#windows).\n",
    "* Install GPy by opening the Command Prompt window and typing the following command:\n",
    "```\n",
    "pip install GPy\n",
    "```\n",
    "* All the dependencies required for running the code are either available in Anaconda or installed using GPy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# import python modules\n",
    "import GPy\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# call matplotlib with the inline command to make plots appear within the browser\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1 Covariance Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPy supports a wide range of covariance functions. Each function is suitable for some applications. In this section, we work with the covariance functions in GPy. Let's start with defining an squared exponential or rbf covariance function in one dimension:\n",
    "$$\n",
    "k(\\mathbf{x},\\mathbf{x'}) = \\sigma^2 \\text{exp} \\left( - \\frac{\\|\\mathbf{x} - \\mathbf{x'} \\|}{2l^2} \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# The documentation to use the RBF function. There are several advanced options such as useGPU which are \n",
    "# important for practical applications. The \"?\" symbol can be used with any function or class to view its\n",
    "# documentation\n",
    "GPy.kern.RBF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input dimension\n",
    "d = 1          \n",
    "# variance\n",
    "var = 1.       \n",
    "# lengthscale\n",
    "length = 0.2\n",
    "\n",
    "# define the kernel\n",
    "k = GPy.kern.RBF(d, variance=var, lengthscale=length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A summary of the kernel can be obtained using the command `print k`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# view the parameters of the covariance function\n",
    "print k"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to plot the kernel as a function of one of its inputs (whilst fixing the other) with `k.plot()`. Use \"?\" to view the properties of the plot function for `kern` instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# plot the covariance function\n",
    "k.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Covariance Function Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The value of the covariance function parameters can be accessed and modified using `k.*` where the * can refer to the parameter name as it appears in `print k`. We'll now set the lengthscale of the covariance to different values, and then plot the resulting covariance using the `k.plot()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# by default, all the parameters are set to 1. for the RBF kernel\n",
    "k = GPy.kern.RBF(d)     \n",
    "\n",
    "# we experiment with different length scale parameter values here\n",
    "theta = np.asarray([0.2,0.5,1.,2.,4.])\n",
    "\n",
    "# create an instance of a figure\n",
    "fig = plt.figure()\n",
    "ax = plt.subplot(111)\n",
    "\n",
    "# iterate over the lengthscales\n",
    "for t in theta:\n",
    "    k.lengthscale=t\n",
    "    # plot in the same figure with a different color\n",
    "    k.plot(ax=ax, color=np.random.rand(3,), plot_limits=[-10.0,10.0])\n",
    "plt.legend(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What is the effect of the lengthscale parameter on the covariance function?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Now change the code used above for plotting the covariances associated with the length scale to see the influence of the variance parameter. What is the effect of the the variance parameter on the covariance function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Instead of rbf, try constructing and plotting the following  covariance functions: `exponential`, `Matern32`, `Matern52`, `Brownian`, `linear`, `bias`, `rbfcos`, `periodic_Matern32`, etc. Use the `tab` key to look for the functions `GPy.kern` submodule or search in the [GPy documentation](http://pythonhosted.org/GPy/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing the kernel matrix given input data, $\\mathbf{X}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let $\\mathbf{X}$ be a $n$ Ã— $d$ numpy array. Given a kernel $k$, the covariance matrix associated to\n",
    "$\\mathbf{X}$ is obtained with `C = k.K(X,X)` . The positive semi-definiteness of $k$ ensures that `C`\n",
    "is a positive semi-definite (psd) matrix regardless of the initial points $\\mathbf{X}$. This can be\n",
    "checked numerically by looking at the eigenvalues:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input data: 50*2 matrix of iid standard Gaussians\n",
    "X = np.random.rand(50,2)       \n",
    "\n",
    "# create the matern52 kernel\n",
    "k = GPy.kern.Matern52(input_dim=2)\n",
    "\n",
    "# compute the kernel matrix\n",
    "C = k.K(X,X)\n",
    "\n",
    "# computes eigenvalues of matrix\n",
    "eigvals = np.linalg.eigvals(C) \n",
    "\n",
    "# plot the eigen values\n",
    "plt.bar(np.arange(len(eigvals)), eigvals)\n",
    "plt.title('Eigenvalues of the Matern 5/2 Covariance')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combining Covariance Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In GPy you can easily combine covariance functions you have created using the sum and product operators, `+` and `*`. For example, we can combine an exponentiated quadratic covariance with a Matern 5/2 as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define rbf and matern52 kernels\n",
    "kern1 = GPy.kern.RBF(1, variance=1., lengthscale=2.)\n",
    "kern2 = GPy.kern.Matern52(1, variance=2., lengthscale=4.)\n",
    "\n",
    "# combine both kernels\n",
    "kern = kern1 + kern2\n",
    "print kern\n",
    "kern.plot(plot_limits=[-7,9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to multiply two kernel functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "kern = kern1*kern2\n",
    "print kern\n",
    "kern.plot(plot_limits=[-6,8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Sampling from a Gaussian Process"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gaussian process provides a prior over an infinite dimensional function. It is defined by a covariance *function* and a mean *function*. When we compute the covariance matrix using `kern.K(X, X)` i.e. compute a covariance *matrix* between the values of the function that correspond to the input locations in the matrix `X`. Using this we can draw sample paths from a Gaussian process:\n",
    "\n",
    "$$\n",
    "\\mathbf{f} = \\mathcal{N}(\\mathbf{X}|\\mathbf{0},C)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# define RBF kernel\n",
    "k = GPy.kern.RBF(input_dim=1,lengthscale=0.2)\n",
    "\n",
    "# define X to be 500 points evenly spaced over [0,1]\n",
    "X = np.linspace(0.,1.,500) \n",
    "\n",
    "# make the numpy array to 2D array \n",
    "X = X[:,None] \n",
    "\n",
    "# set mean function i.e. 0 everywhere\n",
    "mu = np.zeros((500)) \n",
    "\n",
    "# compute covariance matrix associated with inputs X\n",
    "C = k.K(X,X) \n",
    "\n",
    "# Generate 20 separate samples paths from a Gaussian with mean mu and covariance C\n",
    "Z = np.random.multivariate_normal(mu,C,20)\n",
    "\n",
    "# open a new plotting window\n",
    "fig = plt.figure()     \n",
    "for i in range(20):\n",
    "    plt.plot(X[:],Z[i,:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the structure of the covariance matrix we are plotting from if we visualize C."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.matshow(C)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Try a range of different covariance functions and values to plot the corresponding sample paths for each using the same approach given above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Can you tell the covariance structures that have been used for generating the\n",
    "sample paths shown in the figure below?\n",
    "<br>\n",
    "<center>\n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figa.png\" alt=\"Figure a\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figb.png\" alt=\"Figure b\" style=\"width: 30%;\"> \n",
    "<img src=\"http://ml.dcs.shef.ac.uk/gpss/gpws14/figd.png\" alt=\"Figure d\" style=\"width: 30%;\">\n",
    "</center>\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 Gaussian Process Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will combine the Gaussian process prior with data to form a GP regression model with GPy. We will generate data from the function $f ( x ) = âˆ’ \\cos(\\pi x ) + \\sin(4\\pi x )$ over $[0, 1]$, adding some noise to give $y(x) = f(x) + \\epsilon$, with the noise being Gaussian distributed, $\\epsilon \\sim \\mathcal{N}(0, 0.01)$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# input data points\n",
    "X = np.linspace(0.05,0.95,10)[:,None]\n",
    "# generate observations through function f\n",
    "Y = -np.cos(np.pi*X) + np.sin(4*np.pi*X) + np.random.normal(loc=0.0, scale=0.1, size=(10,1)) \n",
    "\n",
    "# plot the generated data\n",
    "plt.figure()\n",
    "plt.plot(X,Y,'kx',mew=1.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A GP regression model is defined by first specifying the covariance function for analysis. Then an instance of the model is generated with a default set of parameters. Then it is possible to view the parameters using `print m` and visualize the posterior mean prediction and variances using `m.plot`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# create instance of kernel\n",
    "k = GPy.kern.RBF(input_dim=1, variance=1., lengthscale=1.)\n",
    "\n",
    "# create instance of GP regression model\n",
    "m = GPy.models.GPRegression(X,Y,k)\n",
    "\n",
    "# view model parameters\n",
    "print m\n",
    "\n",
    "# visualize posterior mean and variances\n",
    "m.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The actual predictions of the model for a set of points `Xstar` can be computed using `m.predict(Xstar)`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# obtain 5 test points\n",
    "Xstar = np.linspace(0.01,0.99,5)[:,None]\n",
    "\n",
    "# predict the output for the test points\n",
    "Ystar, Vstar = m.predict(Xstar)\n",
    "\n",
    "# print results\n",
    "print Ystar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) What do you think about this first fit? Does the prior given by the GP seem to be\n",
    "adapted?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) The parameters of the models can be modified using a regular expression matching the parameters names (for example `m['noise'] = 0.001` ). Change the values of the parameters to obtain a better fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Random sample paths from the conditional GP can be obtained using `np.random.multivariate_normal(mu[:,0],C)` where the mean vector and covariance matrix `mu`, `C` are obtained through the predict function `mu, C = m.predict(Xp,full_cov=True)`. Obtain 10 samples from the posterior sample and plot them alongside the data below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Covariance Function Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parameters values can be estimated by maximizing the likelihood of the observations. Since we donâ€™t want one of the variance to become negative during the optimization, we can constrain all parameters to be positive before running the optimisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.constrain_positive()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can optimize the hyperparameters of the model using the `m.optimize()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "m.optimize()\n",
    "m.plot()\n",
    "print m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Generate random function samples using the optmized model and plot alongside the input data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Modify the kernel used for building the model to investigate its influence on the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4 A Running Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will consider a small example with real world data: data giving the pace of all marathons run at the olympics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load marathon timing dataset\n",
    "data = np.genfromtxt('marathon.csv', delimiter=',')\n",
    "\n",
    "# set the input and output data\n",
    "X = data[:, 0:1]\n",
    "Y = data[:, 1:2]\n",
    "\n",
    "# plot the timings\n",
    "plt.plot(X, Y, 'bx')\n",
    "plt.xlabel('year')\n",
    "plt.ylabel('marathon pace min/km')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "a) Build a Gaussian process model for the olympic data set using a combination of an RBF and a bias covariance function. Fit the covariance function parameters and the noise to the data. Plot the fit and error bars from 1870 to 2030. Do you think the predictions are reasonable? If not why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "b) Fit the same model, but this time intialize the length scale of the RBF kernel to 0.5. What has happened? Which of model has the higher log likelihood, this one or the one from (a)? \n",
    "\n",
    "*Hint:* use `model.log_likelihood()` for computing the log likelihood."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "c) Modify your model by including two covariance functions. Intitialize a covariance function with an exponentiated quadratic part, a Matern 3/2 part and a bias covariance. Set the initial lengthscale of the exponentiated quadratic to 80 years, set the initial length scale of the Matern 3/2 to 10 years. Optimize the new model and plot the fit again. How does it compare with the previous model? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "d) Repeat part c) but now initialize both of the covariance functions' lengthscales to 20 years. Check the model parameters, what happens now? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "e) Now model the data with a product of an exponentiated quadratic covariance function and a linear covariance function. Fit the covariance function parameters. Why are the variance parameters of the linear part so small? How could this be fixed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Answer here"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Answer here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
