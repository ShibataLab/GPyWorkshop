% Slides for lab retreat
% Author: Nishanth Koganti
% Date: 2015/7/29

\documentclass[10pt]{beamer}

\usetheme{CambridgeUS}
\usecolortheme{seahorse}
\usefonttheme{serif}

\usepackage{bm}
\usepackage{tikz}
\usepackage{color}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancybox}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath{{./Images/}}
\usepackage[absolute,overlay]{textpos}
\usetikzlibrary{arrows,shapes,backgrounds,shapes.misc,fit}

\setbeamercovered{invisible}
%\setbeamercovered{transparent}

\newenvironment{reference}[2]{%
  \begin{textblock*}{\textwidth}(#1,#2)
    \footnotesize\it\bgroup\color{red!50!black}}{\egroup\end{textblock*}}

\everymath{\displaystyle}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\gp}{\mathcal{GP}}
\newcommand{\gaussN}{\mathcal{N}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSig}{\boldsymbol{\Sigma}}

\title{Learning with Gaussian Processes using GPy}
\author{Nishanth Koganti}
\date{\today}

\begin{document}

  \begin{frame}[noframenumbering]
    \titlepage
  \end{frame}

  \begin{frame}
    \frametitle{Supervised Learning: Ubiquitous questions}

    \begin{itemize}
      \item Model fitting
      \begin{itemize}
        \item How to fit parameters?
        \item How to handle overfitting?
      \end{itemize}

      \item Model selection
      \begin{itemize}
        \item Which model best represents data?
        \item How sure can I be?
      \end{itemize}

      \item Interpretation
      \begin{itemize}
        \item What is the accuracy of predictions?
        \item Can I trust predictions under model uncertainity?
      \end{itemize}
    \end{itemize}

    \begin{center}
      \textbf{Gaussian Processes provides framework to address these issues.}
    \end{center}
  \end{frame}

  \begin{frame}
    \frametitle{Gaussian Processes: Extremely Short Overview}

    \begin{columns}
      \begin{column}{0.33\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{func1.png}
        \end{figure}
      \end{column}
      \pause
      \begin{column}{0.33\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{func2.png}
        \end{figure}
      \end{column}
      \pause
      \begin{column}{0.33\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{func3.png}
        \end{figure}
      \end{column}
    \end{columns}
  \end{frame}

  \begin{frame}[noframenumbering]
    \frametitle{Outline}
    \tableofcontents
  \end{frame}

  \section{Gaussian Processes}
  \begin{frame}
    \frametitle{Gaussian Distribution}

    \begin{figure}
      \centering
      \includegraphics[width=0.9\textwidth]{gaussDist1.png}
    \end{figure}

    \begin{equation*}
      \begin{array}{c}
        p(\bx|\bmu,\bSig) = \gaussN(\bmu,\bSig) = (2\pi)^{-D/2} |\bSig|^{-1/2} \exp \left( - \frac{1}{2} (\bx - \bmu)^T \bSig^{-1} (\bx - \bmu) \right) \\
        \bmu \text{: mean vector, } \bSig \text{: covariance matrix}
      \end{array}
    \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Conditional and Marginal of a Gaussian}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{gaussDist2.png}
    \end{figure}

    \begin{center}
      \textcolor{blue}{Conditional} and \textcolor{red}{Marginal} of a joint Gaussian is also Gaussian.
    \end{center}

  \end{frame}

  \begin{frame}
    \frametitle{What is a Gaussian Process?}
    Generalization of a multivariate Gaussian to \textcolor{red}{infinitely many variables}.

    \begin{block}{}
      \textbf{Definition}: \emph{Gaussian Process is a collection of random variables, any finite collection of which are Gaussian Distributed.}
    \end{block}

    Gaussian \textcolor{blue}{distribution}: mean \textcolor{red}{vector}, $\bmu$, and covariance \textcolor{red}{matrix} $\bSig$:
    \begin{equation*}
      \mathbf{f} = (f_1,\dots,f_n)^T \sim \gaussN(\bmu,\bSig),~~\text{indices } i = 1,\dots,n
    \end{equation*}

    Gaussian \textcolor{blue}{process}: mean \textcolor{red}{function}, $m(x)$, and covariance \textcolor{red}{function} $k(x,x')$:
    \begin{equation*}
      f(x) \sim \gp(m(x),k(x,x')),~~\text{indices: } x
    \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Marginalization Property}
    How can we represent infinite mean vector and infinite covariance matrix? \\~

    ...luckily saved by \emph{marginalization property}:

    \begin{equation*}
      p(\bx) = \int p(\bx,\by)d \by
    \end{equation*}

    For Gaussians:

    \begin{equation*}
      \begin{array}{c}
        p(\bx, \by) = \gaussN \left(\begin{bmatrix} \ba \\ \bb \end{bmatrix}, \begin{bmatrix} \bA & \bB \\ \bB^T & \bC \end{bmatrix} \right) \\~\\
        p(\bx) = \gaussN(\ba,\bA)
      \end{array}
    \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Random sampling from Gaussian Process}
    Considering one dimensional Gaussian process:

    \begin{equation*}
      p(f(x)) \sim \gp \left( m(x) = 0, k(x,x') = \exp \left( - \frac{1}{2} (x - x')^2 \right) \right)
    \end{equation*}

    Sampling is done by focusing on subset $\boldf = (f(x_1), f(x_2),\dots,f(x_n))^T$:

    \begin{equation*}
      \boldf \sim \gaussN(0,\bSig) \text{, where } \bSig_{ij} = k(x_i,x_j)
    \end{equation*}

    Coordinates of $\boldf$ are plot as a function of corresponding $x$
  \end{frame}

  \begin{frame}
    \frametitle{Gaussian Distribution Sample}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{samplePath.png}
    \end{figure}
  \end{frame}

  \begin{frame}[noframenumbering]
    \frametitle{Gaussian Distribution Sample: $f1$ vs $f2$}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{covariance1.png}
    \end{figure}
  \end{frame}

  \begin{frame}[noframenumbering]
    \frametitle{Gaussian Distribution Sample: $f1$ vs $f5$}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{covariance2.png}
    \end{figure}
  \end{frame}

  \begin{frame}
    \frametitle{Squared Exponential Covariance Function}

    \begin{columns}
      \begin{column}{0.35\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{kernelMatrix.png}
        \end{figure}
      \end{column}

      \begin{column}{0.45\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{samplePaths.png}
        \end{figure}
      \end{column}
    \end{columns}
  \end{frame}

  \section{Inference using Gaussian Processes}

  \begin{frame}
    \frametitle{Parametric Model and Maximum Likelihood}
    Parametric Model:
      \begin{itemize}
        \item data: $\bx, \by$
        \item model: $\by = f_w(\bx) + \epsilon$
      \end{itemize}

    Gaussian Likelihood:
      \begin{equation*}
        \textcolor{red}{p(\by|\bx, \bw, M)} \propto \prod_i \exp \left( - \frac{ (y_i - f_{\bw}(x_i))^2}{2 \sigma_{noise}^2} \right)
      \end{equation*}

    Maximizing Likelihood:
      \begin{equation*}
        \bw_{ML} = \argmax_{\bw} \textcolor{red}{p(\by|\bx,\bw,M)}
      \end{equation*}

    Making predictions:
      \begin{equation*}
        p(y^*|x^*,\bw_{ML},M)
      \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Parametric Model and Bayesian Inference}
    Parametric Model:
      \begin{itemize}
        \item data: $\bx, \by$
        \item model: $\by = f_w(\bx) + \epsilon$
      \end{itemize}

    Gaussian Likelihood:
      \begin{equation*}
        \textcolor{red}{p(\by|\bx, \bw, M)} \propto \prod_i \exp \left( - \frac{ (y_i - f_{\bw}(x_i))^2}{2 \sigma_{noise}^2} \right)
      \end{equation*}

    Prior over parameters:
      \begin{equation*}
        \textcolor{blue}{p(\bw|M)}
      \end{equation*}

    Posterior parameter distribution:
      \begin{equation*}
        \textcolor{green}{p(\bw|\bx,\by,M)} = \frac{\textcolor{blue}{p(\bw|M)} \textcolor{red}{p(\by|\bx, \bw, M)}}{p(\by|\bx,M)}
      \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Parametric Model and Bayesian Inference}

    Making predictions:
      \begin{equation*}
        p(y^*|x^*,\bx,\by,M) = \int p(y^*|\bw,x^*,M) \textcolor{green}{p(\bw|\bx,\by,M)} d\bw
      \end{equation*}

    Marginal Likelihood:
      \begin{equation*}
        p(\by|\bx,M) = \int \textcolor{blue}{p(\bw|M)} \textcolor{red}{p(\by|\bx, \bw, M)} d\bw
      \end{equation*}

    Model probability:
      \begin{equation*}
        p(M|\bx,\by) = \frac{p(M)p(\by|\bx,M)}{p(\by|\bx)}
      \end{equation*}

    \begin{center}
      \textbf{Problem: integrals are intractable for most interesting models!}
    \end{center}
  \end{frame}

  \begin{frame}
    \frametitle{Non-parametric Gaussian Process Models}
    Parameters are replaced by ``function'' itself!

    Gaussian Likelihood:
      \begin{equation*}
        \textcolor{red}{\by|\bx,f(x),M} \sim \gaussN(\boldf, \sigma^2_{noise}I)
      \end{equation*}

    Gaussian Process Prior:
      \begin{equation*}
        \textcolor{blue}{f(x)|M} \sim \gp(m(x) = 0, k(x,x'))
      \end{equation*}

    Leading to Gaussian Process Posterior:
      \begin{equation*}
        \begin{array}{c}
          \textcolor{green}{f(x)|\bx,\by,M} \sim \gp ( m_{\text{post}}(x) = k(x,\bx)[K(\bx,\bx) + \sigma^2_{noise}I]^{-1} \by, \\[0.2cm]
          \; k_{\text{post}}(x,x') =  k(x,x') - k(x,\bx)[K(\bx,\bx) + \sigma^2_{noise}I]^{-1} k(\bx,x') )
        \end{array}
      \end{equation*}
    \end{frame}

  \begin{frame}
    \frametitle{Prior and Posterior for $\gp$ Learning}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{gpRegression.png}
    \end{figure}

    Gaussian Process Predictive Distribution:
      \begin{equation*}
        \begin{array}{c}
          p(y^*|x^*,\bx,\by) \sim \gaussN ( k(x^*,\bx)[K + \sigma^2_{noise}]^{-1}\by, \\[0.2cm]
          \; k(x*,x*) - k(x^*,\bx)[K + \sigma^2_{noise}I]^{-1} k(\bx,x^*) )
        \end{array}
      \end{equation*}
  \end{frame}

  \section{Covariance Functions}

  \begin{frame}
    \frametitle{Matern Covariance Function}

    \begin{equation*}
      k(x,x') = \frac{1}{\Gamma(\nu) 2^{\nu - 1}} \left[ \frac{\sqrt{2 \nu}}{l} |x - x'| \right]^{\nu} K_{\nu} \left( \frac{\sqrt{2 \nu}{l}} |x - x'| \right)
    \end{equation*}
    where $K_{\nu}$ is a Bessel function of order $\nu$, and $l$ is the length scale.\\~

    Samples of Matern forms are $\lfloor \nu - 1 \rfloor$ times differentiable.

    \begin{figure}
      \centering
      \includegraphics[width=0.9\textwidth]{maternCovFunc.png}
    \end{figure}

  \end{frame}

  \begin{frame}
    \frametitle{Periodic Covariance Function}
    Periodic covariance functions can be obtained by mapping $x$ to $u = (\sin(x), \cos(x))^T$ and combine with SE covariance function:

    \begin{equation*}
      k_{periodic} (x,x') = \exp \left( - \frac{2 \sin^2(\pi (x - x'))}{l^2} \right)
    \end{equation*}

    \begin{figure}
      \centering
      \includegraphics[width=0.8\textwidth]{periodicCovFunc.png}
      \caption*{3 random samples with: left $l > 1$ and right $l < 1$}
    \end{figure}
  \end{frame}

  \section{Practical Application}

  \begin{frame}
    \frametitle{Prediction Problem}

    \begin{figure}
      \centering
      \includegraphics[width=0.8\textwidth]{prediction1.png}
    \end{figure}
  \end{frame}

  \begin{frame}
    \frametitle{Covariance Functions}

    \begin{itemize}
      \item long term smooth trend (\textcolor{blue}{squared exponential})
        \begin{equation*}
          k_1(x,x') = \theta_1^2 \exp \left( \frac{(x - x')^2}{\theta_2^2} \right)
        \end{equation*}
      \item seasonal trend (\textcolor{blue}{quasi-periodic smooth})
        \begin{equation*}
          k_2(x,x') = \theta_3^2 \exp \left( - \frac{2 \sin^2 (\pi (x - x'))}{\theta_5^2} \right) \times \exp \left( \frac{(x - x')^2}{2 \theta_4^2} \right)
        \end{equation*}
      \item short and medium term anomaly (\textcolor{blue}{rational quadratic})
        \begin{equation*}
          k_3(x,x') = \theta_6^2 \left( 1 + \frac{(x - x')^2}{2 \theta_8 \theta_7^2} \right)^{- \theta_8}
        \end{equation*}
    \end{itemize}

    \begin{equation*}
      k(x,x') = k_1(x,x') + k_2(x,x') + k_3(x,x') + \text{noise kernel}
    \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Carbon Dioxide Predictions}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{prediction2.png}
    \end{figure}
  \end{frame}

  \begin{frame}
    \frametitle{Long and Medium-term Predictions}

    \begin{figure}
      \centering
      \includegraphics[width=0.8\textwidth]{prediction3.png}
    \end{figure}
  \end{frame}

  \section{Conclusions}

  \begin{frame}
    \frametitle{Conclusions}

    \textbf{Complex non-linear inference problems can be solved by manipulating plain old Gaussian Distributions}

    \begin{itemize}
      \item Bayesian inference is tractable for GP Regression
      \item Predictions are probabilistic
    \end{itemize}

    \textbf{Scope for research:}

    \begin{itemize}
      \item Interesting covariance functions
      \item Application to high-dimensional data (Deep Learning)
    \end{itemize}

  \end{frame}

  \section{Appendix}

  \begin{frame}[noframenumbering]
    \frametitle{Optimizing Marginal Likelihood}
    \begin{equation*}
      \log p(\by|\bx,M) = - \textcolor{blue}{\frac{1}{2} \by^T K^{-1} \by} - \textcolor{red}{\frac{1}{2} \log|K|} - \frac{n}{2} \log (2\pi)
    \end{equation*}

    is a combination of \textcolor{blue}{data fit} and \textcolor{red}{complexity penalty} terms. Occam's razor is automatic!\\~\\

    \textcolor{blue}{Learning} in Gaussian process models involves finding:
      \begin{itemize}
        \item Form of covariance matrix
        \item Unknown hyperparameter values $\theta$
      \end{itemize}

    ~\\ This can be done by optimizing the marginal likielihood:

    \begin{equation*}
      \frac{\partial \log p(\by|\bx,\theta,M)}{\partial \theta_j} = \frac{1}{2} \by^T K^{-1} \frac{\partial K}{\partial \theta_j} K^{-1} \by - \frac{1}{2} \text{trace} \left( K^{-1} \frac{\partial K}{\partial \theta_j} \right)
    \end{equation*}
  \end{frame}

  \begin{frame}[noframenumbering]
    \frametitle{Example: Length Parameter Learning}

    Covariance function: $k(x,x') = \nu^2 \exp \left( - \frac{(x - x')^2}{2l^2} \right) + \sigma_{noise}^2 \delta_{xx'}$

    \begin{figure}
      \centering
      \includegraphics[width=0.8\textwidth]{gpLearning.png}
    \end{figure}

    Posterior mean function is plotted for 3 different length scales, green curve maximizes marginal likelihood. \textcolor{blue}{Although exact fit for data can be found, marginal likelihood does not favour this!}
  \end{frame}

  \begin{frame}[noframenumbering]
    \frametitle{Why does Bayesian Inference work?: Occam's Razor}

    \begin{figure}
      \centering
      \includegraphics[width=0.9\textwidth]{occamsRazor.png}
    \end{figure}
  \end{frame}
\end{document}
