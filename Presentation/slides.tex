% Slides for GPy workshop
% Author: Nishanth Koganti
% Date: 2017/4/4

\documentclass[10pt]{beamer}

\usetheme{CambridgeUS}
\usecolortheme{seahorse}
\usefonttheme{serif}

\usepackage{bm}
\usepackage{tikz}
\usepackage{color}
\usepackage{xcolor}
\usepackage{caption}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{fancybox}
\usepackage{amsfonts}
\usepackage{listings}
\usepackage{graphicx}
\usepackage{subcaption}
\graphicspath{{./Images/}}
\usepackage[absolute,overlay]{textpos}
\usetikzlibrary{arrows,shapes,backgrounds,shapes.misc,fit,positioning}

\setbeamercovered{invisible}
%\setbeamercovered{transparent}
\setbeamerfont{footnote}{size=\tiny}
\setbeamerfont{caption}{size=\tiny}

\newenvironment{reference}[2]{%
  \begin{textblock*}{\textwidth}(#1,#2)
    \footnotesize\it\bgroup\color{red!50!black}}{\egroup\end{textblock*}}

\everymath{\displaystyle}

\newcommand{\bx}{\mathbf{x}}
\newcommand{\by}{\mathbf{y}}
\newcommand{\ba}{\mathbf{a}}
\newcommand{\bb}{\mathbf{b}}
\newcommand{\bw}{\mathbf{w}}
\newcommand{\boldf}{\mathbf{f}}
\newcommand{\bA}{\mathbf{A}}
\newcommand{\bB}{\mathbf{B}}
\newcommand{\bC}{\mathbf{C}}
\newcommand{\gp}{\mathcal{GP}}
\newcommand{\gaussN}{\mathcal{N}}
\newcommand{\argmax}{\text{argmax}}
\newcommand{\bmu}{\boldsymbol{\mu}}
\newcommand{\bSig}{\boldsymbol{\Sigma}}

\title[GPs with GPy]{Learning with Gaussian Processes using GPy}
\author[Nishanth]{Nishanth Koganti}
\date{\today}

\begin{document}

  \begin{frame}[noframenumbering]
    \titlepage
  \end{frame}

  \begin{frame}[noframenumbering]
    \frametitle{Outline}
    \tableofcontents
  \end{frame}

  \section{Introduction}

  \begin{frame}
    \frametitle{Resources}

    \begin{columns}
      \begin{column}{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=0.6\textwidth]{gpml.png}
          \caption*{Gaussian Processes for Machine Learning \footnotemark[1]}
        \end{figure}
      \end{column}
      \begin{column}{0.5\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=0.4\textwidth]{gpss.png}
          \caption*{Gaussian Process Summer School \footnotemark[2]}
        \end{figure}
        \vskip-2em
        \begin{figure}
          \centering
          \includegraphics[width=0.5\textwidth]{gpy.png}
          \caption*{GPy Library \footnotemark[3]}
        \end{figure}
      \end{column}
    \end{columns}
    \footnotetext[1]{Gaussian Processes for Machine Learning, C. Williams, C. Rasmussen}
    \footnotetext[2]{Gaussian Process Summer Schools, \url{http://gpss.cc/}}
    \footnotetext[3]{GPy Library, \url{https://github.com/SheffieldML/GPy}}
  \end{frame}

  \begin{frame}
    \frametitle{Gaussian Processes: Extremely Short Overview}

    \begin{columns}
      \begin{column}{0.33\textwidth}
        Generate functions
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{func1.png}
        \end{figure}
      \end{column}
      \pause
      \begin{column}{0.33\textwidth}
        Observe Data
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{func2.png}
        \end{figure}
      \end{column}
      \pause
      \begin{column}{0.33\textwidth}
        Remove invalid functions
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{func3.png}
        \end{figure}
      \end{column}
    \end{columns}
  \end{frame}

  \subsection{GPy Library}

  \section{Gaussian Processes}

  \begin{frame}
    \frametitle{What is a Gaussian Process?}
    Generalization of a multivariate Gaussian to \textcolor{red}{infinitely many variables}.

    \begin{block}{}
      \textbf{Definition}: \emph{Gaussian Process is a collection of random variables, any finite collection of which are Gaussian Distributed.}
    \end{block}

    Gaussian \textcolor{blue}{distribution}: mean \textcolor{red}{vector}, $\bmu$, and covariance \textcolor{red}{matrix} $\bSig$:
    \begin{equation*}
      \mathbf{f} = (f_1,\dots,f_n)^T \sim \gaussN(\bmu,\bSig),~~\text{indices } i = 1,\dots,n
    \end{equation*}

    Gaussian \textcolor{blue}{process}: mean \textcolor{red}{function}, $m(x)$, and covariance \textcolor{red}{function} $k(x,x')$:
    \begin{equation*}
      f(x) \sim \gp(m(x),k(x,x')),~~\text{indices: } x
    \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Marginalization Property}
    How can we represent infinite mean vector and infinite covariance matrix? \\~

    ...luckily saved by \emph{marginalization property}:

    \begin{equation*}
      p(\bx) = \int p(\bx,\by)d \by
    \end{equation*}

    For Gaussians:

    \begin{equation*}
      \begin{array}{c}
        p(\bx, \by) = \gaussN \left(\begin{bmatrix} \ba \\ \bb \end{bmatrix}, \begin{bmatrix} \bA & \bB \\ \bB^T & \bC \end{bmatrix} \right) \\~\\
        p(\bx) = \gaussN(\ba,\bA)
      \end{array}
    \end{equation*}
  \end{frame}

  \begin{frame}
    \frametitle{Random sampling from Gaussian Process}
    Considering one dimensional Gaussian process:

    \begin{equation*}
      p(f(x)) \sim \gp \left( m(x) = 0, k(x,x') = \exp \left( - \frac{1}{2} (x - x')^2 \right) \right)
    \end{equation*}

    Sampling is done by focusing on subset $\boldf = (f(x_1), f(x_2),\dots,f(x_n))^T$:

    \begin{equation*}
      \boldf \sim \gaussN(0,\bSig) \text{, where } \bSig_{ij} = k(x_i,x_j)
    \end{equation*}

    Coordinates of $\boldf$ are plot as a function of corresponding $x$
  \end{frame}

  \subsection{Covariance Functions}

  \begin{frame}
    \frametitle{Matern Covariance Function}

    \begin{equation*}
      k(x,x') = \frac{1}{\Gamma(\nu) 2^{\nu - 1}} \left[ \frac{\sqrt{2 \nu}}{l} |x - x'| \right]^{\nu} K_{\nu} \left( \frac{\sqrt{2 \nu}{l}} |x - x'| \right)
    \end{equation*}
    where $K_{\nu}$ is a Bessel function of order $\nu$, and $l$ is the length scale.\\~

    Samples of Matern forms are $\lfloor \nu - 1 \rfloor$ times differentiable.

    \begin{figure}
      \centering
      \includegraphics[width=0.9\textwidth]{maternCovFunc.png}
    \end{figure}

  \end{frame}

  \begin{frame}
    \frametitle{Squared Exponential Covariance Function}

    \begin{equation*}
      k(\mathbf{x},\mathbf{x}') = \alpha \text{exp} \left( - \frac{\|\mathbf{x} - \mathbf{x}'\|^2}{2l^2} \right)
    \end{equation*}

    where $\alpha$ is the variance and $\l$ is the length scale of the covariance function
    \begin{columns}
      \begin{column}{0.35\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{kernelMatrix.png}
        \end{figure}
      \end{column}

      \begin{column}{0.45\textwidth}
        \begin{figure}
          \centering
          \includegraphics[width=\textwidth]{samplePaths.png}
        \end{figure}
      \end{column}
    \end{columns}
  \end{frame}

  \subsection{Gaussian Process Regression}

  \begin{frame}
    \frametitle{Gaussian Process Regression}
    Parameters are replaced by ``function'' itself!

    Gaussian Likelihood:
      \begin{equation*}
        \textcolor{red}{\by|\bx,f(x),M} \sim \gaussN(\boldf, \sigma^2_{noise}I)
      \end{equation*}

    Gaussian Process Prior:
      \begin{equation*}
        \textcolor{blue}{f(x)|M} \sim \gp(m(x) = 0, k(x,x'))
      \end{equation*}

    Leading to Gaussian Process Posterior:
      \begin{equation*}
        \begin{array}{c}
          \textcolor{green}{f(x)|\bx,\by,M} \sim \gp ( m_{\text{post}}(x) = k(x,\bx)[K(\bx,\bx) + \sigma^2_{noise}I]^{-1} \by, \\[0.2cm]
          \; k_{\text{post}}(x,x') =  k(x,x') - k(x,\bx)[K(\bx,\bx) + \sigma^2_{noise}I]^{-1} k(\bx,x') )
        \end{array}
      \end{equation*}
    \end{frame}

  \begin{frame}
    \frametitle{Prior and Posterior for $\gp$ Learning}

    \begin{figure}
      \centering
      \includegraphics[width=\textwidth]{gpRegression.png}
    \end{figure}

    Gaussian Process Predictive Distribution:
      \begin{equation*}
        \begin{array}{c}
          p(y^*|x^*,\bx,\by) \sim \gaussN ( k(x^*,\bx)[K + \sigma^2_{noise}]^{-1}\by, \\[0.2cm]
          \; k(x*,x*) - k(x^*,\bx)[K + \sigma^2_{noise}I]^{-1} k(\bx,x^*) )
        \end{array}
      \end{equation*}
  \end{frame}

  \section{Dimensionality Reduction with GP}

  \begin{frame}
  \frametitle{Non-linear Dimensionality Reduction}

  \begin{center}
    \textbf{UPSC Handwritten Digit Dataset}
  \end{center}

  \begin{columns}[t]
    \begin{column}[t]{0.3\textwidth}
      \centering
      3648 dimensional space
      \begin{figure}
        \centering
        \caption*{Digit 6 Image}
        \includegraphics[width=0.3\textwidth]{digitIm1.png}
      \end{figure}
      \begin{figure}
        \centering
        \caption*{Random Image}
        \includegraphics[width=0.3\textwidth]{digitIm2.png}
      \end{figure}
    \end{column}
    \begin{column}{0.6\textwidth}
      \centering
      Low-dimensional manifold for digit rotation
      \begin{figure}
        \centering
        \includegraphics[width=0.8\textwidth]{pca1.png}
      \end{figure}
    \end{column}
  \end{columns}

  \end{frame}

  \begin{frame}
  \frametitle{Probabilistic Generative Model}

  \begin{itemize}
    \item \textcolor{blue}{Observed} (high-dimensional) data: $\mathbf{Y} = [y_1~y_2~\cdots~y_N]^T \in \mathbb{R}^{N \times D}$
    \item \textcolor{blue}{Latent} (low-dimensional) data: $\mathbf{X} = [x_1~x_2~\cdots~x_N]^T \in \mathbb{R}^{N \times Q},~Q << D$
    \item Assume a relationship/mapping of the form:
      \begin{equation}
        \begin{array}{c}
          y_i = \mathbf{W}x_i + \epsilon_i,~\epsilon_i \sim \mathcal{N}(\mathbf{0},\sigma^2\mathbf{I})\\~\\
          y_i = f(x_i) = \epsilon_i
        \end{array}
      \end{equation}
    \item Resultant likelihood on the data:
      \begin{equation}
        p(\mathbf{Y}|\mathbf{X},\mathbf{W}) = \prod_{i = 1}^N \mathcal{N}(y_i | \mathbf{W}x_i, \sigma^2\mathbf{I})
      \end{equation}
  \end{itemize}

  \end{frame}

  \begin{frame}
  \frametitle{Probabilistic Generative Model}

  \begin{columns}
    \begin{column}[t]{0.48\textwidth}
      \centering
      \textbf{Probabilistic PCA}
      \begin{figure}
        \centering
        \begin{tikzpicture}
          \tikzstyle{main}=[circle, minimum size = 6mm, thick, draw =black!80, node distance = 12mm]
          \tikzstyle{main1}=[circle, minimum size = 6mm, thick, draw = white, node distance = 12mm]
          \tikzstyle{connect}=[-latex, thick]
          \node[main,fill=black!40] (y) [label=center:$\mathbf{Y}$] {};
          \node[main] (x) [above=of y,label=center:$\mathbf{X}$] {};
          \node[main1] (w) [left=of y,label=center:$\mathbf{W}$] {};
          \node[main1] (sigma) [right=of y,label=center:$\sigma^2$] {};
          \path (x) edge [connect] (y)
          (w) edge [connect] (y)
          (sigma) edge [connect] (y);
        \end{tikzpicture}
      \end{figure}
      Places prior on latent space $\mathbf{X}$ and optimises linear mapping $\mathbf{W}$
      \begin{equation}
        \begin{array}{c}
          p(\mathbf{X}) = \prod_{i=1}^N \mathcal{N}(x_i|\mathbf{0},\mathbf{I})\\~\\
          p(\mathbf{Y}|\mathbf{W},\sigma^2) = \int p(\mathbf{Y}|\mathbf{W},\mathbf{X},\sigma^2)~p(\mathbf{X})
        \end{array}
      \end{equation}
    \end{column}
    \begin{column}[t]{0.48\textwidth}
      \centering
      \textbf{Dual Probabilistic PCA}
      \begin{figure}
        \centering
        \begin{tikzpicture}
          \tikzstyle{main}=[circle, minimum size = 6mm, thick, draw = black!80, node distance = 12mm]
          \tikzstyle{main1}=[circle, minimum size = 6mm, thick, draw = white, node distance = 12mm]
          \tikzstyle{connect}=[-latex, thick]
          \node[main,fill=black!30] (y) [label=center:$\mathbf{Y}$] {};
          \node[main] (w) [above=of y,label=center:$\mathbf{W}$] {};
          \node[main1] (x) [left=of y,label=center:$\mathbf{X}$] {};
          \node[main1] (sigma) [right=of y,label=center:$\sigma^2$] {};
          \path (x) edge [connect] (y)
          (w) edge [connect] (y)
          (sigma) edge [connect] (y);
        \end{tikzpicture}
      \end{figure}
      Places prior on linear mapping $\mathbf{W}$ and optimises latent space $\mathbf{X}$
      \begin{equation*}
        \begin{array}{c}
          p(\mathbf{W}) = \prod_{i=1}^D \mathcal{N}(w_i|\mathbf{0},\mathbf{I})\\~\\
          p(\mathbf{Y}|\mathbf{X},\sigma^2) = \int p(\mathbf{Y}|\mathbf{W},\mathbf{X},\sigma^2)~p(\mathbf{W})
        \end{array}
      \end{equation*}
    \end{column}
  \end{columns}

  \end{frame}

  \begin{frame}
  \frametitle{From Dual PPCA to GP-LVM}

  \begin{exampleblock}{}
    \centering
    PPCA and Dual PPCA are equivalent eigenvalue problems with same Maximum Likelihood solution
  \end{exampleblock}

  \begin{itemize}
    \item \textcolor{blue}{GP-LVM}: Instead of placing prior $p(\mathbf{W})$ on the function parameters in Dual PPCA, we can place a prior $p(f)$ directly on the mapping function i.e. \textcolor{blue}{$\mathcal{GP}$ Prior}\\~\\
    \item A $\mathcal{GP}$ Prior allows for \textcolor{blue}{non-linear mappings} if the covariance function is non-linear. For example, the SE Covariance Function:
      \begin{equation}
        k(x,x') = \alpha~\text{exp} \left( - \frac{\gamma}{2} (x - x')^T(x - x') \right)
      \end{equation}
  \end{itemize}

  \begin{figure}
    \centering
    \begin{tikzpicture}
      \tikzstyle{main}=[circle, minimum size = 6mm, thick, draw = black!80, node distance = 12mm]
      \tikzstyle{main1}=[circle, minimum size = 6mm, thick, draw = white, node distance = 12mm]
      \tikzstyle{connect}=[-latex, thick]
      \node[main,fill=black!30] (y) [label=center:$\mathbf{Y}$] {};
      \node[main] (f) [above=of y,label=center:$f$] {};
      \node[main1] (theta) [right=of f,label=center:$\theta$] {};
      \node[main1] (x) [left=of y,label=center:$\mathbf{X}$] {};
      \node[main1] (sigma) [right=of y,label=center:$\sigma^2$] {};
      \path (x) edge [connect] (y)
      (f) edge [connect] (y)
      (sigma) edge [connect] (y)
      (theta) edge [connect] (f);
    \end{tikzpicture}
  \end{figure}

  \end{frame}

  \begin{frame}
  \frametitle{Linear vs. Non-linear Dimensionality Reduction}

  \begin{figure}
    \centering
    \includegraphics[width=0.8\textwidth]{mapping.png}
  \end{figure}

  \end{frame}

  \begin{frame}
  \frametitle{Extensions of GP-LVM}

  \textbf{Back Constrained GP-LVM}: Ensures points close in the observation space ($Y$) will be close in latent space by constraining back mapping $f': Y \rightarrow X$\\~\\

  \textbf{GP-LVM with Dynamics Model}: Computes latent space assuming that the latent positions ($\mathbf{X}$) are sequential:

  \begin{equation}
    x_t = h(x_{t-1}) + \epsilon_{dyn}, \epsilon_{dyn} \sim \mathcal{N}(\mathbf{0},\sigma^2_{dyn}\mathbf{I})
  \end{equation}

  A $\mathcal{GP}$ Prior is placed on the function $h(x)$. The resultant optimization becomes:

  \begin{equation}
    \mathbf{X},\theta,\theta_{dyn} = \text{argmax}_{\mathbf{X},\theta,\theta_{dyn}}~p(\mathbf{Y}|\mathbf{X},\theta)~p(\mathbf{X}|\theta_{dyn})
  \end{equation}
  \end{frame}

  \subsection{BGPLVM}

  \section{GPflow Library}


\end{document}
